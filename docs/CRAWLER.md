# Press Release Crawler Documentation

ê³¼ê¸°ë¶€(MSIT)ì™€ ë°©í†µìœ„(KCC) ë³´ë„ìë£Œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” í¬ë¡¤ë§ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

ì´ ì‹œìŠ¤í…œì€ ë‹¤ìŒ ë‘ ê¸°ê´€ì˜ ë³´ë„ìë£Œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤:

1. **ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ (MSIT)**: https://www.msit.go.kr/bbs/list.do?sCode=user&mPid=208&mId=307
2. **ë°©ì†¡í†µì‹ ìœ„ì›íšŒ (KCC)**: https://www.kcc.go.kr/user.do?boardId=1113&page=A05030000&dc=K05030000

## ğŸ—ï¸ ì‹œìŠ¤í…œ êµ¬ì¡°

```
lib/crawler/
â”œâ”€â”€ types.ts          # íƒ€ì… ì •ì˜
â”œâ”€â”€ base.ts           # ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
â”œâ”€â”€ msit.ts           # ê³¼ê¸°ë¶€ í¬ë¡¤ëŸ¬
â”œâ”€â”€ kcc.ts            # ë°©í†µìœ„ í¬ë¡¤ëŸ¬
â”œâ”€â”€ service.ts        # í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)
â””â”€â”€ index.ts          # ëª¨ë“ˆ export

app/api/
â”œâ”€â”€ crawl/            # ìˆ˜ë™ í¬ë¡¤ë§ API
â””â”€â”€ press-releases/   # ë³´ë„ìë£Œ ì¡°íšŒ API

supabase/migrations/
â””â”€â”€ 002_press_releases.sql  # DB ìŠ¤í‚¤ë§ˆ
```

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

```sql
CREATE TABLE press_releases (
  id BIGSERIAL PRIMARY KEY,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

  -- ì†ŒìŠ¤ ì •ë³´
  source VARCHAR(50) NOT NULL,        -- 'msit' ë˜ëŠ” 'kcc'
  source_id VARCHAR(255) NOT NULL,    -- ì›ë³¸ ì‚¬ì´íŠ¸ì˜ ID

  -- ì½˜í…ì¸ 
  title TEXT NOT NULL,
  content TEXT,
  published_at TIMESTAMP WITH TIME ZONE NOT NULL,
  url TEXT NOT NULL,

  -- ë©”íƒ€ë°ì´í„°
  category VARCHAR(100),
  department VARCHAR(200),
  author VARCHAR(200),
  attachments JSONB,

  -- ì¶”ì 
  last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

  -- ì¤‘ë³µ ë°©ì§€
  UNIQUE(source, source_id)
);
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env.local` íŒŒì¼ì— ë‹¤ìŒ ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:

```bash
# Supabase
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
```

### 2. ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜

Supabase ëŒ€ì‹œë³´ë“œì˜ SQL Editorì—ì„œ `supabase/migrations/002_press_releases.sql` íŒŒì¼ì˜ ë‚´ìš©ì„ ì‹¤í–‰í•˜ì„¸ìš”.

### 3. ë¡œì»¬ í…ŒìŠ¤íŠ¸

í¬ë¡¤ëŸ¬ë¥¼ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:

```bash
npm run test-crawler
```

ì´ ëª…ë ¹ì–´ëŠ”:
- ê° í¬ë¡¤ëŸ¬ì—ì„œ ìµœëŒ€ 5ê°œì˜ ë³´ë„ìë£Œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
- ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤
- ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### 1. ìˆ˜ë™ í¬ë¡¤ë§ (ë³´ë„ìë£Œ ìˆ˜ì§‘)

**ì—”ë“œí¬ì¸íŠ¸**: `GET /api/crawl`

**ì„¤ëª…**: ê³¼ê¸°ë¶€ì™€ ë°©í†µìœ„ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ë³´ë„ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.

**ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°**:
- `limit` (ì„ íƒ): ì†ŒìŠ¤ë‹¹ ìˆ˜ì§‘í•  ìµœëŒ€ í•­ëª© ìˆ˜ (ê¸°ë³¸ê°’: 20)

**ì‚¬ìš© ì˜ˆì‹œ**:
```bash
# ê¸°ë³¸ (ì†ŒìŠ¤ë‹¹ 20ê°œ í•­ëª©)
curl http://localhost:3000/api/crawl

# ì†ŒìŠ¤ë‹¹ 10ê°œ í•­ëª©ë§Œ ìˆ˜ì§‘
curl http://localhost:3000/api/crawl?limit=10

# POST ìš”ì²­ë„ ì§€ì›
curl -X POST http://localhost:3000/api/crawl
```

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "success": true,
  "timestamp": "2024-12-06T10:30:00.000Z",
  "results": [
    {
      "success": true,
      "source": "msit",
      "items_fetched": 20,
      "items_new": 5,
      "items_updated": 2,
      "timestamp": "2024-12-06T10:30:00.000Z"
    },
    {
      "success": true,
      "source": "kcc",
      "items_fetched": 20,
      "items_new": 3,
      "items_updated": 1,
      "timestamp": "2024-12-06T10:30:00.000Z"
    }
  ],
  "totals": {
    "fetched": 40,
    "new": 8,
    "updated": 3,
    "errors": 0
  }
}
```

### 2. ë³´ë„ìë£Œ ì¡°íšŒ

**ì—”ë“œí¬ì¸íŠ¸**: `GET /api/press-releases`

**ì„¤ëª…**: ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ë³´ë„ìë£Œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

**ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°**:
- `source` (ì„ íƒ): 'msit' ë˜ëŠ” 'kcc' - íŠ¹ì • ì†ŒìŠ¤ë§Œ ì¡°íšŒ
- `limit` (ì„ íƒ): ì¡°íšŒí•  ìµœëŒ€ í•­ëª© ìˆ˜ (ê¸°ë³¸ê°’: 50)

**ì‚¬ìš© ì˜ˆì‹œ**:
```bash
# ëª¨ë“  ë³´ë„ìë£Œ (ìµœëŒ€ 50ê°œ)
curl http://localhost:3000/api/press-releases

# ê³¼ê¸°ë¶€ ë³´ë„ìë£Œë§Œ ì¡°íšŒ
curl http://localhost:3000/api/press-releases?source=msit

# ë°©í†µìœ„ ë³´ë„ìë£Œ 10ê°œë§Œ ì¡°íšŒ
curl http://localhost:3000/api/press-releases?source=kcc&limit=10

# ìµœì‹  100ê°œ ì¡°íšŒ
curl http://localhost:3000/api/press-releases?limit=100
```

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "success": true,
  "count": 50,
  "data": [
    {
      "id": 1,
      "source": "msit",
      "source_id": "12345",
      "title": "AI ë°˜ë„ì²´ ê°œë°œ ì§€ì› ì •ì±… ë°œí‘œ",
      "content": "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ëŠ”...",
      "published_at": "2024-12-06T09:00:00Z",
      "url": "https://www.msit.go.kr/...",
      "category": "ì •ì±…",
      "department": "ì •ë³´í†µì‹ ì •ì±…ì‹¤",
      "created_at": "2024-12-06T10:00:00Z",
      "last_updated": "2024-12-06T10:00:00Z"
    }
  ]
}
```

## ğŸ”„ ì¼ë°˜ì ì¸ ì‚¬ìš© íë¦„

### 1. ìµœì´ˆ ìˆ˜ì§‘
```bash
# 1. ë³´ë„ìë£Œ ìˆ˜ì§‘
curl http://localhost:3000/api/crawl

# 2. ìˆ˜ì§‘ëœ ë°ì´í„° í™•ì¸
curl http://localhost:3000/api/press-releases?limit=10
```

### 2. ì •ê¸°ì  ì—…ë°ì´íŠ¸
```bash
# ì£¼ê¸°ì ìœ¼ë¡œ ìƒˆë¡œìš´ ë³´ë„ìë£Œ ìˆ˜ì§‘
curl http://localhost:3000/api/crawl?limit=20
```

### 3. í”„ë¡ íŠ¸ì—”ë“œ í†µí•©
```typescript
// ë³´ë„ìë£Œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
async function getPressReleases() {
  const response = await fetch('/api/press-releases?limit=20');
  const data = await response.json();
  return data.data;
}

// ìƒˆ ë³´ë„ìë£Œ ìˆ˜ì§‘
async function collectPressReleases() {
  const response = await fetch('/api/crawl?limit=10');
  const result = await response.json();
  return result.totals;
}
```

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

1. **ìë™ ì¤‘ë³µ ì œê±°**: ì´ë¯¸ ìˆ˜ì§‘ëœ ë³´ë„ìë£ŒëŠ” ìŠ¤í‚µ
2. **ì½˜í…ì¸  ë³€ê²½ ê°ì§€**: ì œëª©, ë‚´ìš©, ì¹´í…Œê³ ë¦¬, ì²¨ë¶€íŒŒì¼ ë³€ê²½ ì‹œ ìë™ ì—…ë°ì´íŠ¸
3. **ì—ëŸ¬ í•¸ë“¤ë§**: ì¼ë¶€ ì‹¤íŒ¨í•´ë„ ë‚˜ë¨¸ì§€ ê³„ì† ì²˜ë¦¬
4. **Rate Limiting**: ì„œë²„ ë¶€í•˜ ë°©ì§€ (500ms ë”œë ˆì´)
5. **ìˆ˜ë™ ì‹¤í–‰**: í•„ìš”í•  ë•Œë§Œ í¬ë¡¤ë§ ì‹¤í–‰ (ë¹„ìš© ì ˆê°)

## ğŸ”§ í”„ë¡œê·¸ë˜ë° ë°©ì‹ ì‚¬ìš©

```typescript
import { getCrawlerService } from '@/lib/crawler';
import { getSupabaseDB } from '@/lib/supabase/client';

// í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ ì‚¬ìš©
const service = getCrawlerService();

// ë³´ë„ìë£Œ ìˆ˜ì§‘
const results = await service.crawlAll(20); // ê° ì†ŒìŠ¤ì—ì„œ ìµœëŒ€ 20ê°œ í•­ëª©

// Supabaseì—ì„œ ì§ì ‘ ì¡°íšŒ
const db = getSupabaseDB();
const releases = await db.getPressReleases({
  source: 'msit',
  limit: 10,
});
```

## ğŸ” ë¬¸ì œ í•´ê²°

### HTML ì…€ë ‰í„° ì˜¤ë¥˜

ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ë©´ í¬ë¡¤ëŸ¬ê°€ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°:

1. ë¸Œë¼ìš°ì €ì—ì„œ í•´ë‹¹ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì—´ì–´ HTML êµ¬ì¡°ë¥¼ í™•ì¸
2. `lib/crawler/msit.ts` ë˜ëŠ” `lib/crawler/kcc.ts`ì˜ ì…€ë ‰í„° ìˆ˜ì •
3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰: `npm run test-crawler`

### ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨

```bash
# Supabase ì—°ê²° í…ŒìŠ¤íŠ¸
npx tsx scripts/test-supabase.ts
```

### API í˜¸ì¶œ ì‹¤íŒ¨

1. í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (`.env.local`)
2. Supabase ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸

## âš ï¸ ì°¸ê³ ì‚¬í•­

### Vercel Cron Jobs (ë¹„í™œì„±í™”ë¨)

ìë™ ìŠ¤ì¼€ì¤„ë§ ê¸°ëŠ¥ì€ Vercel Pro í”Œëœì´ í•„ìš”í•˜ë¯€ë¡œ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ëŒ€ì‹  ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ ìˆ˜ë™ ì‹¤í–‰í•˜ì„¸ìš”:

1. **ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ í˜¸ì¶œ**: `https://your-domain.vercel.app/api/crawl`
2. **ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**: `npm run test-crawler`
3. **ì™¸ë¶€ ìŠ¤ì¼€ì¤„ëŸ¬**: GitHub Actions, ë‹¤ë¥¸ ì„œë²„ì˜ cron ë“±

### GitHub Actions ì˜ˆì‹œ (ì„ íƒì‚¬í•­)

`.github/workflows/crawl.yml`:
```yaml
name: Crawl Press Releases
on:
  schedule:
    - cron: '0 */6 * * *'  # 6ì‹œê°„ë§ˆë‹¤
  workflow_dispatch:  # ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Trigger crawl API
        run: |
          curl -X POST https://your-domain.vercel.app/api/crawl
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [Cheerio ë¬¸ì„œ](https://cheerio.js.org/)
- [Supabase ë¬¸ì„œ](https://supabase.com/docs)
- [Next.js API Routes](https://nextjs.org/docs/app/building-your-application/routing/route-handlers)
